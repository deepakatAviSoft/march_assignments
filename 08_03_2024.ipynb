{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepakatAviSoft/march_assignments/blob/main/08_03_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_hT3My8nW5l",
        "outputId": "46ba2641-c8de-4bfb-866e-860f0a312a58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "# installing spark using spark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**>>> Load the Walmart Stock CSV File, have Spark infer the data types.**"
      ],
      "metadata": {
        "id": "QZxZ2-_fnXjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the SparkSession module from the pyspark.sql package\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Creating a SparkSession named \"Walmart Analysis\" using the SparkSession builder\n",
        "spark = SparkSession.builder.appName(\"Walmart Analysis\").getOrCreate()\n",
        "\n",
        "# Reading a CSV file named \"walmart_stock.csv\" into a DataFrame named stock_df.\n",
        "# Specifying that the file has a header and inferring the schema automatically.\n",
        "stock_df = spark.read.csv(\"walmart_stock.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Displaying the DataFrame\n",
        "stock_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8uQI7zspgGM",
        "outputId": "fbe98172-00bd-4fc6-ed8d-7163cc9c0abe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Date: date, Open: double, High: double, Low: double, Close: double, Volume: int, Adj Close: double]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. What are the column names?**"
      ],
      "metadata": {
        "id": "9hNLtl96sLzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accessing the columns attribute of the DataFrame stock_df\n",
        "# This attribute returns a list of column names in the DataFrame.\n",
        "stock_df.columns\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOopG_aEsRFt",
        "outputId": "53f9f866-527a-462b-c866-3ab201084c7b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. What does the Schema look like?**"
      ],
      "metadata": {
        "id": "mmSIOFVmsXnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the schema of the DataFrame stock_df\n",
        "# This provides information about the data types of each column in the DataFrame.\n",
        "stock_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhh_V0NBsayL",
        "outputId": "d1b793e6-1d71-418c-930f-aefa3e8a1353"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Print out the first 5 columns.**"
      ],
      "metadata": {
        "id": "aDDvuno_t7Bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting the first five columns of the DataFrame stock_df and displaying the first 5 rows.\n",
        "# The select() function is used to specify the columns to be selected.\n",
        "# Here, stock_df.columns[:5] selects the first five column names.\n",
        "# The show(5) function is used to display the first 5 rows of the resulting DataFrame.\n",
        "first_5 = stock_df.select(stock_df.columns[:5]).show(5)\n",
        "\n",
        "# Printing the result of the show() function, which displays the first 5 rows of the DataFrame.\n",
        "# The show() function returns None, so it doesn't need to be printed.\n",
        "print(first_5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNH05v5BttUb",
        "outputId": "262acdd7-31ef-4111-d1f2-1be8c490494d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+---------+---------+------------------+\n",
            "|      Date|              Open|     High|      Low|             Close|\n",
            "+----------+------------------+---------+---------+------------------+\n",
            "|2012-01-03|         59.970001|61.060001|59.869999|         60.330002|\n",
            "|2012-01-04|60.209998999999996|60.349998|59.470001|59.709998999999996|\n",
            "|2012-01-05|         59.349998|59.619999|58.369999|         59.419998|\n",
            "|2012-01-06|         59.419998|59.450001|58.869999|              59.0|\n",
            "|2012-01-09|         59.029999|59.549999|58.919998|             59.18|\n",
            "+----------+------------------+---------+---------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Use describe() to learn about the DataFrame.**"
      ],
      "metadata": {
        "id": "0uBCg74zu_NP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating summary statistics for numerical columns in the DataFrame stock_df and displaying the results.\n",
        "# The describe() function computes statistics such as count, mean, stddev, min, and max for each numerical column.\n",
        "# The show() function is used to display the summary statistics in a tabular format.\n",
        "stock_df.describe().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wII-E700vDXE",
        "outputId": "12d65855-5713-4571-e96d-6b34bf286951"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|summary|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|  count|              1258|             1258|             1258|             1258|             1258|             1258|\n",
            "|   mean| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
            "| stddev|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
            "|    min|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
            "|    max|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. There are too many decimal places for mean and stddev in the describe() dataframe. Format the numbers to just show up to two decimal places. Pay careful attention to the datatypes that .describe() returns, we didn't cover how to do this exact formatting, but we covered something very similar.**\n"
      ],
      "metadata": {
        "id": "isE76q6GveQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the format_number function from the pyspark.sql.functions module.\n",
        "from pyspark.sql.functions import format_number\n",
        "\n",
        "# Generating summary statistics for the DataFrame stock_df.\n",
        "descr = stock_df.describe()\n",
        "\n",
        "# Selecting specific columns from the summary statistics DataFrame and formatting them.\n",
        "# The format_number function is used to format numerical columns to two decimal places.\n",
        "# The alias function is used to rename the formatted columns.\n",
        "formatted = descr.select(\n",
        "    descr['summary'],\n",
        "    format_number(descr['Open'].cast('float'), 2).alias('Open'),\n",
        "    format_number(descr['High'].cast('float'), 2).alias('High'),\n",
        "    format_number(descr['Low'].cast('float'), 2).alias('Low'),\n",
        "    format_number(descr['Close'].cast('float'), 2).alias('Close'),\n",
        "    format_number(descr['Volume'].cast('float'), 2).alias('Volume')\n",
        ").show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3vOdO7Hvm-q",
        "outputId": "3786fad3-5141-40fd-a5bc-bc1f98291eb3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+--------+--------+--------+-------------+\n",
            "|summary|    Open|    High|     Low|   Close|       Volume|\n",
            "+-------+--------+--------+--------+--------+-------------+\n",
            "|  count|1,258.00|1,258.00|1,258.00|1,258.00|     1,258.00|\n",
            "|   mean|   72.36|   72.84|   71.92|   72.39| 8,222,093.50|\n",
            "| stddev|    6.77|    6.77|    6.74|    6.76| 4,519,781.00|\n",
            "|    min|   56.39|   57.06|   56.30|   56.42| 2,094,900.00|\n",
            "|    max|   90.80|   90.97|   89.25|   90.47|80,898,096.00|\n",
            "+-------+--------+--------+--------+--------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Create a new dataframe with a column called HV Ratio that is the ratio of the High Price versus volume of stock traded for a day**"
      ],
      "metadata": {
        "id": "h2EotNczzBHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the col function from the pyspark.sql.functions module.\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Creating a new column 'HV Ratio' in the DataFrame stock_df.\n",
        "# This column represents the ratio of High to Volume.\n",
        "hv_ratio = stock_df.withColumn('HV Ratio', col('High') / col('Volume'))\n",
        "\n",
        "# Selecting only the 'HV Ratio' column from the DataFrame hv_ratio and displaying it.\n",
        "hv_ratio.select(\"HV Ratio\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toOe39LfzHz1",
        "outputId": "a500a507-6d47-4457-e1a8-a28c50ce7a26"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|            HV Ratio|\n",
            "+--------------------+\n",
            "|4.819714653321546E-6|\n",
            "|6.290848613094555E-6|\n",
            "|4.669412994783916E-6|\n",
            "|7.367338463826307E-6|\n",
            "|8.915604778943901E-6|\n",
            "|8.644477436914568E-6|\n",
            "|9.351828421515645E-6|\n",
            "| 8.29141562102703E-6|\n",
            "|7.712212102001476E-6|\n",
            "|7.071764823529412E-6|\n",
            "|1.015495466386981E-5|\n",
            "|6.576354146362592...|\n",
            "| 5.90145296180676E-6|\n",
            "|8.547679455011844E-6|\n",
            "|8.420709512685392E-6|\n",
            "|1.041448341728929...|\n",
            "|8.316075414862431E-6|\n",
            "|9.721183814992126E-6|\n",
            "|8.029436027707578E-6|\n",
            "|6.307432259386365E-6|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. What day had the Peak High in Price?**"
      ],
      "metadata": {
        "id": "oVL4tnRL1RrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the desc function from the pyspark.sql.functions module.\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "# Sorting the DataFrame stock_df by the 'High' column in descending order.\n",
        "# Then, selecting only the 'Date' column.\n",
        "# The first() function retrieves the first row of the DataFrame after sorting.\n",
        "# [0] is used to access the first element (Date) of the first row.\n",
        "Highest_price_day = stock_df.orderBy(desc(\"High\")).select(\"Date\").first()[0]\n",
        "\n",
        "# Printing the Date of the day with the highest price.\n",
        "Highest_price_day\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmtvtRLp1X3j",
        "outputId": "f94ab3ee-c879-43c9-d2d9-2431d618bb1c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.date(2015, 1, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. What is the max and min of the Volume column?**"
      ],
      "metadata": {
        "id": "ruhNGWJL3vV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the max and min functions from the pyspark.sql.functions module.\n",
        "from pyspark.sql.functions import max, min\n",
        "\n",
        "# Selecting the maximum and minimum values of the 'Volume' column from the DataFrame stock_df.\n",
        "# The max() function calculates the maximum value of the 'Volume' column,\n",
        "# and the min() function calculates the minimum value of the 'Volume' column.\n",
        "max_min_vol = stock_df.select(max(\"Volume\"), min(\"Volume\"))\n",
        "\n",
        "# Displaying the maximum and minimum values of the 'Volume' column.\n",
        "max_min_vol.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP5Mbbtr3v98",
        "outputId": "f2317f64-f25a-4900-fa07-9d77a1de7092"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "|max(Volume)|min(Volume)|\n",
            "+-----------+-----------+\n",
            "|   80898100|    2094900|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. How many days was the Close lower than 60 dollars?**"
      ],
      "metadata": {
        "id": "YX9Cg1Zi4ZCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the col function from the pyspark.sql.functions module.\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Filtering the DataFrame stock_df to count the number of days where the 'Close' price is less than 60.\n",
        "# The filter() function is used with the condition col(\"Close\") < 60.\n",
        "# The count() function calculates the number of rows that satisfy the condition.\n",
        "days_lower_close_60 = stock_df.filter(col(\"Close\") < 60).count()\n",
        "\n",
        "# Printing the number of days where the 'Close' price is less than 60.\n",
        "print(days_lower_close_60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG2g00Zy4l-z",
        "outputId": "8a939faa-7cd0-41c0-eacf-fda7de2b23cc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. What percentage of the time was the High greater than 80 dollars?**"
      ],
      "metadata": {
        "id": "91Xg4xbG5mId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the col function from the pyspark.sql.functions module.\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Filtering the DataFrame stock_df to count the number of days where the 'High' price is greater than 80.\n",
        "Higher_price_80 = stock_df.filter(col(\"High\") > 80).count()\n",
        "\n",
        "# Calculating the total number of days in the DataFrame stock_df.\n",
        "total = stock_df.count()\n",
        "\n",
        "# Calculating the percentage of days where the 'High' price is greater than 80.\n",
        "# The percentage is calculated as (count of days with High > 80) / (total count of days) * 100.\n",
        "percentage = (Higher_price_80 / total) * 100\n",
        "\n",
        "# Rounding the percentage value to 6 decimal places and printing it.\n",
        "print(round(percentage, 6))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWttJ42d5qt5",
        "outputId": "cca31386-29f6-4cc1-ca9c-3d1068248d35"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.141494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. What is the max High per year**"
      ],
      "metadata": {
        "id": "S9ky-jui64ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the max and year functions from the pyspark.sql.functions module.\n",
        "from pyspark.sql.functions import max, year\n",
        "\n",
        "# Creating a new column 'Year' in the DataFrame stock_df to extract the year from the 'Date' column.\n",
        "# The year() function extracts the year from the 'Date' column.\n",
        "df_Years = stock_df.withColumn('Year', year(stock_df['Date']))\n",
        "\n",
        "# Grouping the DataFrame df_Years by the 'Year' column and aggregating the maximum 'High' price for each year.\n",
        "# The agg() function is used to apply the aggregation function max() to the 'High' column and aliasing it as 'MaxHighAllYears'.\n",
        "highest_of_all_years = df_Years.groupBy('Year').agg(max('High').alias('MaxHighAllYears'))\n",
        "\n",
        "# Displaying the DataFrame containing the highest 'High' price for each year.\n",
        "highest_of_all_years.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0Vz03Eq68Dq",
        "outputId": "ab66d73e-1a2f-4f85-e4c2-d211cc1736df"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------+\n",
            "|Year|MaxHighAllYears|\n",
            "+----+---------------+\n",
            "|2015|      90.970001|\n",
            "|2013|      81.370003|\n",
            "|2014|      88.089996|\n",
            "|2012|      77.599998|\n",
            "|2016|      75.190002|\n",
            "+----+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. What is the Pearson correlation between High and Volume?**"
      ],
      "metadata": {
        "id": "X9YvmiAe_jxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the corr function from the pyspark.sql.functions module.\n",
        "from pyspark.sql.functions import corr\n",
        "\n",
        "# Calculating the correlation between the 'High' and 'Volume' columns in the DataFrame stock_df.\n",
        "# The corr() function computes the correlation coefficient between two columns.\n",
        "correlation_high_vol = stock_df.corr('High', 'Volume')\n",
        "\n",
        "# Printing the correlation coefficient between the 'High' and 'Volume' columns.\n",
        "correlation_high_vol\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnqU1CMx_pH5",
        "outputId": "7febfd63-5f4d-485d-9ddb-155368139cd3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.3384326061737161"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13.** What is the average Close for each Calendar Month?\n",
        "In other words, across all the years, what is the average Close price for Jan,Feb, Mar, etc... Your result will have a value for each of these months.\n"
      ],
      "metadata": {
        "id": "rNpK1UWKAt6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the month and avg functions from the pyspark.sql.functions module.\n",
        "from pyspark.sql.functions import month, avg\n",
        "\n",
        "# Creating a new column 'Months' in the DataFrame stock_df to extract the month from the 'Date' column.\n",
        "# The month() function extracts the month from the 'Date' column.\n",
        "df_month = stock_df.withColumn('Months', month(stock_df['Date']))\n",
        "\n",
        "# Grouping the DataFrame df_month by the 'Months' column and calculating the average 'Close' price for each month.\n",
        "# The agg() function is used to apply the aggregation function avg() to the 'Close' column\n",
        "# and aliasing it as 'Average Closing Price'.\n",
        "new = df_month.groupby('Months').agg(avg('Close').alias('Average Closing Price'))\n",
        "\n",
        "# Ordering the resulting DataFrame monthly_closing_price by the 'Months' column.\n",
        "monthly_closing_price = new.orderBy('Months')\n",
        "\n",
        "# Displaying the DataFrame containing the average closing price for each month.\n",
        "monthly_closing_price.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFbTfIP5BJ77",
        "outputId": "b2f4fdcb-fa4c-44be-d0d4-4c0592ddcc55"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------------------+\n",
            "|Months|Average Closing Price|\n",
            "+------+---------------------+\n",
            "|     1|    71.44801958415842|\n",
            "|     2|      71.306804443299|\n",
            "|     3|    71.77794377570092|\n",
            "|     4|    72.97361900952382|\n",
            "|     5|    72.30971688679247|\n",
            "|     6|     72.4953774245283|\n",
            "|     7|    74.43971943925233|\n",
            "|     8|    73.02981855454546|\n",
            "|     9|    72.18411785294116|\n",
            "|    10|    71.57854545454543|\n",
            "|    11|     72.1110893069307|\n",
            "|    12|    72.84792478301885|\n",
            "+------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ski5QRYPwANR"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}